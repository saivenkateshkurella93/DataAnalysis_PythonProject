# PythonProject
Information Retrieval - Web pages Document Spawning - Document Analysis
We discuss the method for implementing agglomerative clustering for the documents, based on TF-IDF score from HTML documents. Further, the discussion continues about its performance and comparisons based on the tokenization method and overall time complexity is shown. The explanation of code and the process is discussed in a greater detail. 


The data available in raw format can have multiple dimensions of representation, which is accessible in random order. Thus, efficient setup of data into a representation is always been a challenge in Information Retrieval field. The Information Retrieval domain deals with scenarios where we store data in the disk with a representation, when query is fired, this representation helps to retrieve the data efficiently. This efficiency can be in terms of retrieval speed, storage required, ranking and recommendation and many other tasks. The ultimate task of the information retrieval is to get the data, send the requested information to the user which is relevant to the query in minimum amount of time.

TF-IDF, is one of the ways of ranking the data which is available in raw format. The user will send some query to the intelligent system which will try to retrieve the relevant information from the representation. TF-IDF is a way of representation which will give the document ranking based on the queries from user. The TFIDF score not only looks for the word inside the document, but also compute the contribution overall dataset of documents. Thus, TF is the term frequency within the document, sometimes we normalize this value so that we can have equal contribution of the words. We can use multiple methods to normalize the frequencies of the word, one way is to compute the L1-norm or L2-norm across the document. We can also apply whatsoever distance formula or error magnification and manipulation formula available and represents bests scores. It is true that we can compute TF and IDFs for all the terms or token within the document also we can compute the TF-IDF score by applying the product. The IDF scores can be computed by looking over all the documents which has a specific token and computing log of the inverse probability of the token. The IDF scores are intuitive as we are also interested to the contribution of the document over the dataset. Thus, we product both signals that is, TF scores which is within document score and IDF score which are inter-document score and integrate them by using product to get TF-IDF score for the tokens. 

We will compare the results based on the tokenization process and the overall time to compute the TF-IDF scores. We can limit the extra words from the list of tokens which are appearing very rare.  Some words are appearing countable time such as for 1 to 9 times, we can ignore those terms for computing the scores for computation acceleration. However, we cannot ignore the contribution of those words, as user may fire the query for those specific words where system fails, but they are numerically insignificant we taken into consideration. Thus, it depends upon the situation and what you need to maximize, for example if a user is a lawyer or a doctor and expecting a document with a specific word, in such situation every word contribution is important to plot the data to user, on other hand, if the user is looking for a word which are very frequent in such situation we can think of chopping the Zipf’s tail. 

The term indexing and its methods are used to have better representation of the documents when query is performed. The locations of document and the frequency of the word within the document in maintained by this method. There are several methods for the computing the index and corresponding posting list. Sometimes, we use the position to locate the document and another method is to store the counts or TFIDF scores.

The agglomerative clustering is the way to cluster the items, or documents according to the similarity score computed within the samples provided. To represent the document in the feature space, we select TFIDF as feature dimensions. We pad TFIDF scores with zeros to make it of same length and define the linkage between documents based on similarity score. The similarity score that we are having for the experiment is Cosine similarity. For the next step, we will do the average of the two-cosine similarity vector. The agglomerative clustering will be performed over this similarity scores.

METHODOLOGY 

When we start the procedure, we first give the number of documents that we need to take into consideration for computing the TF-IDF scores, as we are performing the batch processing. In batch processing, we assume that the data is appearing in a single batch, and we have the access of all the data provided as that batch. Once the batch is acquired, we will process the fragments in the batch and tokenize the data based on 3 parametric regular expressions, that is Upper-case and Lower-case characters, numbers and punctuations marks. First the sentence is tokenized, then the corresponding words and in last the characters. 
 



After tokenization, the term frequency of the tokens is computed with in the document, thus we will have TF scores for all the documents. This TF-score is computed by computing the frequency of the tokens within the document and normalizing with sum of the frequencies of the tokens within that single document. To do this, we utilize the functionality of Counter library in python which is a high-performance counter and results in dictionary, thus we will have one dictionary for every file given as input. After TF score, IDF scores are computed across the document, for that we will maintain the list of keys from the TF dictionaries and compute the frequency of the tokens appearing in that list. By doing so, we can compute the number of times a particular token appearing within series of documents. This dictionary generated is computed once throughout the process. 

Next step is to compute the final TF-IDF scores, we can do so by computing the product between the TF-score of the token and the corresponding IDF score for the token. Thus, we have an iteration module which will iterate over all the tokens within the documents and will compute the log to base 10 of the products of TF-scores and the inverse probability of the token appearing in the document. 


Finally, once the scores are generated, they are saved to the disk, for one input file we will have one output file. We will extract the name of the file from the input and save the output files with same name. We use .wts extention for saving the file. The data file consists the dictionary or the hash table, with its Key: Token and Value: TFIDF score stored. 	

Now after computing the scores of the TFIDF, we will use them to process the query, the query is presented in the same way we do preprocess. The query provided is tokenized, and then passed to retrieve method. The constraint of this method is that we need the preprocessed TFIDF scores. Here, we use the brute force method to find the matching documents. Process goes like following:

•	Load weights files to RAM (form a dictionary of keys as tokens and value as TFIDF scores)
•	Get the query and tokenize the same way as they process the data.
•	Get the weights for the query words in list from user
•	We do brute force search within all dictionary of the documents and score the documents based on the weights and query provide
•	We rank the documents with respect to score computed and sort them in descending order.
•	Perform agglomerative clustering of all the documents and create a dendogram implementing the clustering.
•	Cease the clustering when no two clusters (or documents) have similarity greater than 0.4. 
